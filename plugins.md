# CDAP Plugins Reference

## Condition

|                                                                                                                                                                             Description                                                                                                                                                                              |Display Name|   Name    |  Type   |
|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------|-----------|---------|
|A control flow plugin that allows conditional execution within pipelines. The conditions are specified as expressions and the variables could include values specified as runtime arguments of the pipeline, token from plugins prior to the condition and global that includes global information about pipeline like stage, pipeline, logical start time and plugin.|Conditional |Conditional|Condition|


## Transform

|                                                                                                                                                                                  Description                                                                                                                                                                                  |         Display Name          |             Name              |  Type   |
|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------|-------------------------------|---------|
|Executes user-provided python code that transforms one record into zero or more records. Each input record is converted into a dictionary which can be directly accessed in python. The transform expects to receive a dictionary as input, which it can process and emit zero or more transformed dictionaries, or emit an error dictionary using the provided emitter object.|PythonEvaluator                |PythonEvaluator                |Transform|
|Executes user-provided JavaScript that transforms one record into zero or more records. Input records are converted into JSON objects which can be directly accessed in JavaScript. The transform expects to receive a JSON object as input, which it can process and emit zero or more records or emit error using the provided emitter object.                               |JavaScript                     |JavaScript                     |Transform|
|The Projection transform lets you drop, keep, rename, and cast fields to a different type. Fields are first dropped based on the drop or keep field, then cast, then renamed.                                                                                                                                                                                                  |Projection                     |Projection                     |Transform|
|Parses logs from any input source for relevant information such as URI, IP, browser, device, HTTP status code, and timestamp.                                                                                                                                                                                                                                                  |LogParser                      |LogParser                      |Transform|
|Transforms a StructuredRecord into an Avro GenericRecord.                                                                                                                                                                                                                                                                                                                      |StructuredRecordToGenericRecord|StructuredRecordToGenericRecord|Transform|
|Validates a record, writing to an error dataset if the record is invalid. Otherwise it passes the record on to the next stage.                                                                                                                                                                                                                                                 |Validator                      |Validator                      |Transform|
|The union splitter is used to split data by a union schema, so that type specific logic can be done downstream.                                                                                                                                                                                                                                                                |UnionSplitter                  |UnionSplitter                  |Transform|
|Makes a copy of every input record received for a configured number of times on the output.  This transform does not change any record fields or types. It's an identity transform.                                                                                                                                                                                            |CloneRecord                    |CloneRecord                    |Transform|
|The XML Parser Transform uses XPath to extract fields from a complex XML event. This plugin should generally be used in conjunction with the XML Reader Batch Source. The XML Reader will provide individual events to the XML Parser, which will be responsible for extracting fields from the events and mapping them to the output schema.                                  |XMLParser                      |XMLParser                      |Transform|
|                                                                                                                                                                                                                                                                                                                                                                               |CSVParser                      |CSVParser                      |Transform|
|                                                                                                                                                                                                                                                                                                                                                                               |Decompressor                   |Decompressor                   |Transform|
|                                                                                                                                                                                                                                                                                                                                                                               |Hasher                         |Hasher                         |Transform|
|                                                                                                                                                                                                                                                                                                                                                                               |Decoder                        |Decoder                        |Transform|
|Parses an input JSON event into a record. The input JSON event could be either a map of string fields to values or it could be a complex nested JSON structure. The plugin allows you to express JSON paths for extracting fields from complex nested input JSON.                                                                                                              |JSONParser                     |JSONParser                     |Transform|
|                                                                                                                                                                                                                                                                                                                                                                               |JSONFormatter                  |JSONFormatter                  |Transform|
|                                                                                                                                                                                                                                                                                                                                                                               |Encoder                        |Encoder                        |Transform|
|The XML Multi Parser Transform uses XPath to extract fields from an XML document. It will generate records from the children of the element specified by the XPath. If there is some error parsing the document or building the record, the problematic input record will be dropped.                                                                                          |XMLMultiParser                 |XMLMultiParser                 |Transform|
|Value Mapper is a transform plugin that maps string values of a field in the input record to a mapping value using a mapping dataset.                                                                                                                                                                                                                                          |ValueMapper                    |ValueMapper                    |Transform|
|Compresses configured fields. Multiple fields can be specified to be compressed using different compression algorithms. Plugin supports SNAPPY, ZIP, and GZIP types of compression of fields.                                                                                                                                                                                  |Compressor                     |Compressor                     |Transform|
|Accepts a field that contains a properly-formatted XML string and  outputs a properly-formatted JSON string version of the data. This is  meant to be used with the Javascript transform for the parsing of  complex XML documents into parts. Once the XML is a JSON string, you  can convert it into a Javascript object using:                                              |XMLToJSON                      |XMLToJSON                      |Transform|
|Encrypts one or more fields in input records using a java keystore  that must be present on all nodes of the cluster.                                                                                                                                                                                                                                                          |Encryptor                      |Encryptor                      |Transform|
|                                                                                                                                                                                                                                                                                                                                                                               |StreamFormatter                |StreamFormatter                |Transform|
|                                                                                                                                                                                                                                                                                                                                                                               |CSVFormatter                   |CSVFormatter                   |Transform|
|                                                                                                                                                                                                                                                                                                                                                                               |NullFieldSplitter              |NullFieldSplitter              |Transform|
|Decrypts one or more fields in input records using a keystore  that must be present on all nodes of the cluster.                                                                                                                                                                                                                                                               |Decryptor                      |Decryptor                      |Transform|
|Normalize is a transform plugin that breaks one source row into multiple target rows. Attributes stored in the columns of a table or a file may need to be broken into multiple records: for example, one record per column attribute. In general, the plugin allows the conversion of columns to rows.                                                                        |Normalize                      |Normalize                      |Transform|
|This plugin applies data transformation directives on your data records. The directives are generated either through an interactive user interface or by manual entry into the plugin.                                                                                                                                                                                         |Wrangler                       |Wrangler                       |Transform|
|Adds a new field to each record. The field value can either be a new UUID, or it can be set directly through configuration. This transform is used when you want to add a unique id field to each record, or when you want to tag each record with some constant value. For example, you may want to add the logical start time as a field to each record.                     |AddField                       |AddField                       |Transform|
|Multi Field Adder Transform allows you to add one or more fields to the output. Each field specified has a name and the value. The value is currently set to be of type string.                                                                                                                                                                                                |MultiFieldAdder                |MultiFieldAdder                |Transform|
|                                                                                                                                                                                                                                                                                                                                                                               |AzureFaceExtractor             |AzureFaceExtractor             |Transform|
|NaN                                                                                                                                                                                                                                                                                                                                                                            |GoldenGateNormalizer           |GoldenGateNormalizer           |Transform|
|This transform takes a date in either a unix timestamp or a string, and converts it to a formatted string. (Macro-enabled)                                                                                                                                                                                                                                                     |DateTransform                  |DateTransform                  |Transform|
|Filters out messages based on a specified criteria.                                                                                                                                                                                                                                                                                                                            |FastFilter                     |FastFilter                     |Transform|
|NaN                                                                                                                                                                                                                                                                                                                                                                            |SpeechTranslator               |SpeechTranslator               |Transform|
|                                                                                                                                                                                                                                                                                                                                                                               |PDFExtractor                   |PDFExtractor                   |Transform|
|Given a field and a delimiter, emits one record for each split of the field.                                                                                                                                                                                                                                                                                                   |RecordSplitter                 |RecordSplitter                 |Transform|
|Runs an executable binary which is installed and available on the local filesystem of the Hadoop nodes. Run transform plugin allows the user to read the structured record as input and returns the output record, to be further processed downstream in the pipeline.                                                                                                         |Run                            |Run                            |Transform|


## Sink

|                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |           Display Name            |              Name              |Type|
|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------|--------------------------------|----|
|Writes records to a KeyValueTable, using configurable fields from input records as the key and value.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |KVTable                            |KVTable                         |Sink|
|Batch sink that writes data to a Cube dataset.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |Cube                               |Cube                            |Sink|
|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |TPFSAvro                           |TPFSAvro                        |Sink|
|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |TPFSParquet                        |TPFSParquet                     |Sink|
|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |TPFSOrc                            |TPFSOrc                         |Sink|
|A batch sink for a PartitionedFileSet that writes snapshots of data as a new partition. Data is written in Text format.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |SnapshotText                       |SnapshotText                    |Sink|
|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |HDFS                               |HDFS                            |Sink|
|Uses a Machine Learning algorithm to train a model. Input data must contain a outcome field and one or more feature fields.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |ModelTrainer                       |ModelTrainer                    |Sink|
|Azure Data Lake Store Batch Sink writes data to Azure Data Lake Store directory in avro, orc or text format.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |ADLSBatchSink                      |ADLSBatchSink                   |Sink|
|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |CDCHBase                           |CDCHBase                        |Sink|
|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |CDCKudu                            |CDCKudu                         |Sink|
|Trains a regression model based upon a particular label and features of a record. Saves this model to a FileSet.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |DecisionTreeTrainer                |DecisionTreeTrainer             |Sink|
|Executes user-provided Spark code in Scala that operates on an input RDD or Dataframe with full access to all Spark features.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |ScalaSparkSink                     |ScalaSparkSink                  |Sink|
|This plugins exports a bigquery table as sink to be ingested into the processing pipeline. Plugin requires a service account to access the bigquery table. In order to configure the service account visit https://cloud.google.com. Make sure you provide right permissions to service account for accessing BigQuery API.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |Google Big Query Table             |BigQueryTable                   |Sink|
|A GCS sink to write records as AVRO records into Parquet files.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |GCSParquet                         |GCSParquet                      |Sink|
|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |GooglePublisher                    |GooglePublisher                 |Sink|
|A GCS sink to write records as AVRO records.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |GCSAvro                            |GCSAvro                         |Sink|
|A GCS sink to write records as Comma, Tab, Pipe, CTRL+A separated or JSON text files.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |GCSText                            |GCSText                         |Sink|
|Sink plugin to send the messages from the pipeline to an external http endpoint.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |HTTP                               |HTTP                            |Sink|
|Kinesis sink that outputs to a specified Amazon Kinesis Stream.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |KinesisSink                        |KinesisSink                     |Sink|
|Trains a classification model based upon a particular label and features of a record. Saves this model to a FileSet.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |LogisticRegressionTrainer          |LogisticRegressionTrainer       |Sink|
|Using a Naive Bayes algorithm, trains a model based upon a particular label and text field of a record. Saves this model to a file in a FileSet dataset.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |NaiveBayesTrainer                  |NaiveBayesTrainer               |Sink|
|Writes data to an OrientDB database.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |OrientDB                           |OrientDB                        |Sink|
|A batch sink for writing to Amazon S3 in Avro format.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |S3Avro                             |S3Avro                          |Sink|
|A batch sink to write to S3 in Parquet format.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |S3Parquet                          |S3Parquet                       |Sink|
|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |Parquet Dynamic Partitioned Dataset|ParquetDynamicPartitionedDataset|Sink|
|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |Avro Dynamic Partitioned Dataset   |AvroDynamicPartitionedDataset   |Sink|
|Writes to a CDAP FileSet in text format. HDFS append must be enabled for this to work. One line is written for each record sent to the sink. All record fields are joined using a configurable separator. Each time a batch is written, the sink will examine all existing files in the output directory. If there are any files that are smaller in size than the size threshold, or more recent than the age threshold, new data will be appended to those files instead of written to new files.                                                                                                                                                                                                                                                                                                                                                                                                                                                          |File Appender                      |FileAppender                    |Sink|
|MapR-DB JSON table sink is used to write the JSON documents to the MapR-DB table.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |MapR-DB JSON Table                 |MapRDBJSON                      |Sink|
|NaN                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |ADLS Batch Sink                    |DynamicMultiADLS                |Sink|
|This plugin is normally used in conjunction with the MultiTableDatabase batch source to write records from multiple databases into multiple filesets in text format. Each fileset it writes to will contain a single 'ingesttime' partition, which will contain the logical start time of the pipeline run. The plugin expects that the filsets it needs to write to will be set as pipeline arguments, where the key is 'multisink.[fileset]' and the value is the fileset schema. Normally, you rely on the MultiTableDatabase source to set those pipeline arguments, but they can also be manually set or set by an Action plugin in your pipeline. The sink will expect each record to contain a special split field that will be used to determine which records are written to each fileset. For example, suppose the the split field is 'tablename'. A record whose 'tablename' field is set to 'activity' will be written to the 'activity' fileset.|DynamicMultiFileset                |DynamicMultiFileset             |Sink|
|Trash consumes all the records on the input and eats them all, means no output is generated or no output is stored anywhere.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |Trash                              |Trash                           |Sink|
|The File Copy plugin is a sink plugin that takes file metadata records as inputs and copies the files into the local HDFS or the local filesystem.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |FileCopySink                       |FileCopySink                    |Sink|
|The S3 File Copy plugin is a sink plugin that takes file metadata records as inputs and copies the files into an Amazon S3 filesystem.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |Amazon S3 Whole File Copier        |S3FileCopySink                  |Sink|


## Source

|                                                                                                                                                                                                                                                                                               Description                                                                                                                                                                                                                                                                                                |         Display Name         |        Name        | Type |
|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------|--------------------|------|
|Batch source for an FTP or SFTP source. Prefix of the path ('ftp://...' or 'sftp://...') determines the source server type, either FTP or SFTP.                                                                                                                                                                                                                                                                                                                                                                                                                                                           |FTP                           |FTP                 |Source|
|File streaming source. Watches a directory and streams file contents of any new files added to the directory. Files must be atomically moved or renamed.                                                                                                                                                                                                                                                                                                                                                                                                                                                  |File                          |File                |Source|
|A batch source that reads from a corresponding SnapshotParquet sink. The source will only read the most recent snapshot written to the sink.                                                                                                                                                                                                                                                                                                                                                                                                                                                              |SnapshotParquet               |SnapshotParquet     |Source|
|The XML Reader plugin is a source plugin that allows users to read XML files stored on HDFS.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |XMLReader                     |XMLReader           |Source|
|The Excel plugin provides user the ability to read data from one or more Excel file(s).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |Excel                         |Excel               |Source|
|A batch source that reads from a corresponding SnapshotAvro sink. The source will only read the most recent snapshot written to the sink.                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |SnapshotAvro                  |SnapshotAvro        |Source|
|Outputs the entire contents of a CDAP Table each batch interval. The Table contents will be refreshed at configurable intervals.                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |Table                         |Table               |Source|
|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |Stream                        |Stream              |Source|
|Batch source that reads from a column family in an HBase table. This source differs from the Table source in that it does not use a CDAP dataset, but reads directly from HBase.                                                                                                                                                                                                                                                                                                                                                                                                                          |HBase                         |HBase               |Source|
|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |HTTPPoller                    |HTTPPoller          |Source|
|Samples tweets in real-time through Spark streaming. Output records will have this schema:                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |Twitter                       |Twitter             |Source|
|Batch source to use Microsoft Azure Blob Storage as a source.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |AzureBlobStore                |AzureBlobStore      |Source|
|Azure Data Lake Store Batch Source reads data from Azure Data Lake Store files and converts it into  StructuredRecord.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |AzureDataLakeStore            |AzureDataLakeStore  |Source|
|Azure Event Hub streaming source. Emits a record with the schema specified by the user. If no schema is specified, it will emit a record with 'message'(bytes).                                                                                                                                                                                                                                                                                                                                                                                                                                           |AzureEventHub                 |AzureEventHub       |Source|
|CDAP plugin for reading from the Kafka topic to which Oracle GoldenGate pushes the change schema and data. This plugin is used in realtime data pipelines only.                                                                                                                                                                                                                                                                                                                                                                                                                                           |CDCDatabase                   |CDCDatabase         |Source|
|NaN                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |Google Cloud Storage File     |GCSFile             |Source|
|NaN                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |Google Cloud Storage File Blob|GCSFileBlob         |Source|
|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |GoogleSubscriber              |GoogleSubscriber    |Source|
|This is a source plugin that allows users to read and process mainframe files.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |MainframeReader               |MainframeReader     |Source|
|Batch source to use Amazon S3 as a Source.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |S3                            |S3                  |Source|
|DynamoDB Batch Source that will read the data items from AWS DynamoDB table and convert each item into the StructuredRecord as per the schema specified by the user, that can be further processed downstream in the pipeline. User can provide the query, to read the items from DynamoDB table.                                                                                                                                                                                                                                                                                                         |DynamoDB                      |DynamoDB            |Source|
|Spark streaming source that reads from AWS Kinesis streams.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |KinesisSource                 |KinesisSource       |Source|
|CDAP Plugin for reading data from Apache Kudu table.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |Kudu                          |Kudu                |Source|
|MapR streaming source. Reads events from MapR stream.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |MapR Stream Consumer          |MapRStream          |Source|
|The MQTT Streaming Source allows you to subscribe to an MQTT broker in a streaming context. You specify the topic to subscribe to as an MQTT client.                                                                                                                                                                                                                                                                                                                                                                                                                                                      |MQTT                          |MQTT                |Source|
|Reads from multiple tables within a database using JDBC. Often used in conjunction with the DynamicMultiFileset sink to perform dumps from multiple tables to HDFS files in a single pipeline. The source will output a record for each row in the tables it reads, with each record containing an additional field that holds the name of the table the record came from. In addition, for each table that will be read, this plugin will set pipeline arguments where the key is 'multisink.[tablename]' and the value is the schema of the table. This is to make it work with the DynamicMultiFileset.|Multiple Database Tables      |MultiTableDatabase  |Source|
|The S3 File Metadata plugin is a source plugin that allows users to read file metadata from an S3 Filesystem.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |S3FileMetadataSource          |S3FileMetadataSource|Source|
|The File Metadata plugin is a source plugin that allows users to read file metadata from a local HDFS or a local filesystem.                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |FileMetadataSource            |FileMetadataSource  |Source|


## Action

|                                                                                                                                                              Description                                                                                                                                                              |          Display Name          |         Name          | Type |
|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------|-----------------------|------|
|Deletes a file or files within an HDFS cluster.                                                                                                                                                                                                                                                                                        |HDFSDelete                      |HDFSDelete             |Action|
|Establishes an SSH connection with remote machine to execute command on that machine.                                                                                                                                                                                                                                                  |SSH                             |SSH                    |Action|
|Sends an email at the end of a pipeline run.                                                                                                                                                                                                                                                                                           |Email                           |Email                  |Action|
|Moves a file or files within an HDFS cluster.                                                                                                                                                                                                                                                                                          |HDFSMove                        |HDFSMove               |Action|
|Copies a file or files on a Microsoft Windows share to an HDFS directory.                                                                                                                                                                                                                                                              |WindowsShareCopy                |WindowsShareCopy       |Action|
|Action that runs a database command.                                                                                                                                                                                                                                                                                                   |Database                        |Database               |Action|
|Runs a database query at the end of the pipeline run. Can be configured to run only on success, only on failure, or always at the end of the run.                                                                                                                                                                                      |DatabaseQuery                   |DatabaseQuery          |Action|
|Performs an HTTP request at the end of a pipeline run.                                                                                                                                                                                                                                                                                 |HTTPCallback                    |HTTPCallback           |Action|
|Executes user-provided Spark code in Scala.                                                                                                                                                                                                                                                                                            |ScalaSparkProgram               |ScalaSparkProgram      |Action|
|Deletes a file or files within ADLS file system.                                                                                                                                                                                                                                                                                       |ADLSDelete                      |ADLSDelete             |Action|
|Executes user-provided Spark code in Python.                                                                                                                                                                                                                                                                                           |PySpark Program                 |PySparkProgram         |Action|
|Copy files from FTP server to the specified destination.                                                                                                                                                                                                                                                                               |FTPCopy                         |FTPCopy                |Action|
|This plugin is used for creating directories on Google Cloud Storage (GCS).                                                                                                                                                                                                                                                            |Google Cloud Storage Path Create|GCSBucketCreate        |Action|
|This plugin is used for deleting directories on Google Cloud Storage (GCS).                                                                                                                                                                                                                                                            |Google Cloud Storage Path Delete|GCSBucketDelete        |Action|
|                                                                                                                                                                                                                                                                                                                                       |Hive Bulk Export                |HiveExport             |Action|
|                                                                                                                                                                                                                                                                                                                                       |Hive Bulk Import                |HiveImport             |Action|
|Action to fetch data from an external http endpoint and create a file in HDFS.                                                                                                                                                                                                                                                         |HTTPToHDFS                      |HTTPToHDFS             |Action|
|                                                                                                                                                                                                                                                                                                                                       |LoadToSnowflake                 |LoadToSnowflake        |Action|
|A Hydrator Action plugin to efficiently export data from Oracle to HDFS or local file system. The plugin uses Oracle's command line tools to export data. The data exported from this tool can then be used in Hydrator pipelines.                                                                                                     |OracleExport                    |OracleExport           |Action|
|                                                                                                                                                                                                                                                                                                                                       |RedshiftToS3                    |RedshiftToS3           |Action|
|S3ToRedshift Action that will load the data from AWS S3 bucket into the AWS Redshift table.                                                                                                                                                                                                                                            |S3ToRedshift                    |S3ToRedshift           |Action|
|                                                                                                                                                                                                                                                                                                                                       |SFTPCopy                        |SFTPCopy               |Action|
|                                                                                                                                                                                                                                                                                                                                       |SFTPDelete                      |SFTPDelete             |Action|
|                                                                                                                                                                                                                                                                                                                                       |SFTPPut                         |SFTPPut                |Action|
|Description                                                                                                                                                                                                                                                                                                                            |StateRestore                    |StateRestore           |Action|
|                                                                                                                                                                                                                                                                                                                                       |ToUTF8                          |ToUTF8                 |Action|
|Bulk exports data in a vertica table into a file.                                                                                                                                                                                                                                                                                      |VerticaBulkExportAction         |VerticaBulkExportAction|Action|
|Vertica Bulk Import Action plugin gets executed after successful mapreduce or spark job. It reads all the files in a given directory and bulk imports contents of those files into vertica table.                                                                                                                                      |VerticaBulkImportAction         |VerticaBulkImportAction|Action|
|Performs an HTTP request some endpoint to get a driver specification. Based on the spec, it will make another call to a nebula endpoint to get data about the dataset, which it will use to set 'input.path', 'input.properties', 'directives', and 'output.schema' arguments that can be used later on in the pipeline through macros.|HTTP Argument Setter            |ArgumentSetter         |Action|
|Performs an HTTP request some endpoint to get a driver specification. Based on the spec, it will make another call to a nebula endpoint to get data about the dataset, which it will use to set 'input.path', 'input.properties', 'directives', and 'output.schema' arguments that can be used later on in the pipeline through macros.|DataFactoryDriver               |DataFactoryDriver      |Action|
|Azure decompress Action plugin decompress gz files from a container on Azure Storage Blob service into another container.                                                                                                                                                                                                              |AzureDecompress                 |AzureDecompress        |Action|
|Azure Delete Action plugin deletes a container on Azure Storage Blob service.                                                                                                                                                                                                                                                          |AzureDelete                     |AzureDelete            |Action|
|                                                                                                                                                                                                                                                                                                                                       |Decompress                      |Decompress             |Action|
|This action plugin can be used to check if a file is empty or if the contents of a file match a given pattern.                                                                                                                                                                                                                         |File Contents Checker           |FileContents           |Action|
|Merges small files within an HDFS cluster.                                                                                                                                                                                                                                                                                             |HDFS File Merge                 |HDFSFileMerge          |Action|
|The Amazon S3 Client Action is used to work with S3 buckets and objects before or after the execution of a pipeline.                                                                                                                                                                                                                   |AmazonS3Client                  |AmazonS3Client         |Action|


## Analytics

|                                                                                                                                                                                                                                                                                                        Description                                                                                                                                                                                                                                                                                                        |        Display Name        |            Name            |  Type   |
|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------|----------------------------|---------|
|The Window plugin is used to window a part of a streaming pipeline.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |Window                      |Window                      |Analytics|
|De-duplicates input records so that all output records are distinct. Can optionally take a list of fields, which will project out all other fields and perform a distinct on just those fields.                                                                                                                                                                                                                                                                                                                                                                                                                            |Distinct                    |Distinct                    |Analytics|
|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |Deduplicate                 |Deduplicate                 |Analytics|
|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |GroupByAggregate            |GroupByAggregate            |Analytics|
|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |Joiner                      |Joiner                      |Analytics|
|Converts raw data into denormalized data based on a key column. User is able to specify the list of fields that should be used in the denormalized record, with an option to use an alias for the output field name. For example, 'ADDRESS' in the input is mapped to 'addr' in the output schema.                                                                                                                                                                                                                                                                                                                         |RowDenormalizer             |RowDenormalizer             |Analytics|
|Uses a model trained by the ModelTrainer plugin to add a prediction field to incoming records. The same features used to train the model must be present in each input record, but input records can also contain additional non-feature fields. If the trained model uses categorical features, and if the record being predicted contains new categories, that record will be dropped. For example, suppose categorical feature 'city' was used to train a model that predicts housing prices. If an incoming record has 'New York' as the city, but 'New York' was not in the training set, that record will be dropped.|MLPredictor                 |MLPredictor                 |Analytics|
|Executes user-provided Spark code in Scala that transforms RDD to RDD with full access to all Spark features.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |ScalaSparkCompute           |ScalaSparkCompute           |Analytics|
|Loads a Decision Tree Regression model from a FileSet and uses it to label the records based on the predicted values.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |DecisionTreePredictor       |DecisionTreePredictor       |Analytics|
|Loads a Logistic Regression model from a file of a FileSet dataset and uses it to classify records.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |LogisticRegressionClassifier|LogisticRegressionClassifier|Analytics|
|Loads a Naive Bayes model from a file of a FileSet dataset and uses it to classify records.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |NaiveBayesClassifier        |NaiveBayesClassifier        |Analytics|
|Calculates statistics for each input field. For every field, a total count and null count will be calculated. For numeric fields, min, max, mean, stddev, zero count, positive count, and negative count will be calculated. For string fields, min length, max length, mean length, and empty count will be calculated. For boolean fields, true and false counts will be calculated. When calculating means, only non-null values are considered.                                                                                                                                                                        |DataProfiler                |DataProfiler                |Analytics|
|This plugins re-partitions a Spark RDD.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |Repartitioner               |Repartitioner               |Analytics|
|Sampling a large dataset flowing through this plugin to pull random records. Supports two types of sampling i.e, Systematic Sampling and Reservoir Sampling.                                                                                                                                                                                                                                                                                                                                                                                                                                                               |Sampling                    |Sampling                    |Analytics|
|Top-N returns the top "n" records from the input set, based on the criteria specified in the plugin configuration.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |TopN                        |TopN                        |Analytics|


## Alert Publisher

|                                                                                                                                                Description                                                                                                                                                |    Display Name     |   Name    |     Type      |
|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------|-----------|---------------|
|Publishes alerts to the CDAP Transactional Messaging System (TMS) as json objects. The plugin allows you to specify the topic and namespace to publish to, as well as a rate limit for the maximum number of alerts to publish per second.                                                                 |TMS                  |TMS        |Alert Publisher|
|Kafka Alert Publisher that allows you to publish alerts to kafka as json objects. The plugin internally uses kafka producer apis to publish alerts.  The plugin allows to specify kafka topic to use for publishing and other additional kafka producer properties. This plugin uses kafka 0.8.2 java apis.|Kafka Alert Publisher|KafkaAlerts|Alert Publisher|


## Error Handler

|                                                                                 Description                                                                                  | Display Name |     Name     |    Type     |
|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------|--------------|-------------|
|The ErrorCollector plugin takes errors emitted from the previous stage and flattens them by adding the error message, code, and stage to the record and outputting the result.|ErrorCollector|ErrorCollector|Error Handler|


## Unknown

|                                                                                                                                             Description                                                                                                                                              |  Display Name  |      Name      | Type  |
|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------|----------------|-------|
|Change Tracking SQL Server Streaming Source allows you to perform EDW offload in realtime through Spark Streaming.  It takes advantage of the change tracking information to minimize the data transfer to keep the downstream dataset  in-sync.                                                      |CTSQLServer     |CTSQLServer     |Unknown|
|Takes the Structured Record from the input source and converts it to a JSON string, then indexes it in Elasticsearch using the index, type, and idField specified by the user. The Elasticsearch server should be running prior to creating the application.                                          |Elasticsearch   |Elasticsearch   |Unknown|
|Hive Import Action executes a hive load statement which loads data from HDFS directory/file location into a hive table.                                                                                                                                                                               |README          |README          |Unknown|
|                                                                                                                                                                                                                                                                                                      |Kafka           |Kafka           |Unknown|
|Kafka sink that allows you to write events into CSV or JSON to kafka. Plugin has the capability to push the data to one or more Kafka topics.  It can use one of the field values from input to partition the data on topic. The sink can also be configured to operate in either sync or async mode. |KAFKAWRITER     |KAFKAWRITER     |Unknown|
|Kafka batch source that emits a records with user specified schema.                                                                                                                                                                                                                                   |KAFKABATCHSOURCE|KAFKABATCHSOURCE|Unknown|
|Kafka streaming source that emits a records with user specified schema.                                                                                                                                                                                                                               |KAFKASOURCE     |KAFKASOURCE     |Unknown|
|CDAP Plugin for reading data from Apache Kudu table.                                                                                                                                                                                                                                                  |KUDU            |KUDU            |Unknown|


